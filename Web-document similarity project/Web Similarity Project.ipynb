{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e50f510-179a-42db-bf1a-046acdba22f1",
   "metadata": {},
   "source": [
    "- Building on your previous Python project\n",
    "- Count the frequency of every word (a word is a sequence of alphanumeric characters, case does NOT matter) in the body of your document\n",
    "- Write a 64 bit hash function for a word using polynomial rolling hash function\n",
    "       hash(s) = s[0] + s[1].p  + s[2].\n",
    "\n",
    "p^2   + ..... + s[n-1].p^(n-1)   mod n       \n",
    "\n",
    "Here s[i] is the ASCII for letter i in a word, use p = 53 and m = p2^64\n",
    "Compute Simhash for the document (as shown in slide 57)\n",
    "Modify your program to take two URLs from the web on the command line, print how many bits are common in their simhashes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e9b6148-d65d-4688-b66b-dacc19965dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# original = \"Gen Gen AI is a predictive language model a translator that sits above existing unstructured data and seeks to generate content that a human would find pleasing. The data sets themselves first need to be rigorously processed and curated Gen, just as data scientists prepare data lakes for advanced analytics and analytical AI\"\n",
    "original = \"Tropical fish include fish found in tropical environments around the world, including both freshwater and salt water species\"\n",
    "original = original.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26e552cb-980f-40ff-8323-ada07109a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', ',', ':', ';',  '/', '\\\\', '!', '?', '(',')', '[', ']', '{', '}', '|', '&', '^', '%', '<', '>', '~', '#']\n",
    "def remove_panctuation(original):\n",
    "    cleaned_data = \"\"\n",
    "    for i in original:\n",
    "        if i not in punctuation:\n",
    "            cleaned_data +=i\n",
    "    return cleaned_data\n",
    "content = remove_panctuation(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be5b1fde-2d08-4371-9fda-6301b51c69f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_frequency(content):\n",
    "    data = content.split()\n",
    "    word_fre_dict = {}\n",
    "    for word in data:\n",
    "        if word not in word_fre_dict:\n",
    "            word_fre_dict[word] = 1\n",
    "        else:\n",
    "            word_fre_dict[word] += 1\n",
    "    return word_fre_dict\n",
    "word_dict = word_frequency(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b637ac12-b9b9-4bb0-b417-b55796cd5a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tropical': (2, '11100010'),\n",
       " 'fish': (2, '01000110'),\n",
       " 'include': (1, '01111100'),\n",
       " 'found': (1, '11101000'),\n",
       " 'in': (1, '00101111'),\n",
       " 'environments': (1, '10010000'),\n",
       " 'around': (1, '11100101'),\n",
       " 'the': (1, '00111001'),\n",
       " 'world': (1, '11110100'),\n",
       " 'including': (1, '11011101'),\n",
       " 'both': (1, '01111001'),\n",
       " 'freshwater': (1, '11111011'),\n",
       " 'and': (1, '01101011'),\n",
       " 'salt': (1, '01111000'),\n",
       " 'water': (1, '11010011'),\n",
       " 'species': (1, '00000100')}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeHashValue(word_dict):\n",
    "    p = 53\n",
    "    m = 2**8\n",
    "    words_with_hash = {}\n",
    "    for word in word_dict:\n",
    "        hash = 0\n",
    "        n=len(word)\n",
    "        for j in range(n):\n",
    "            ascii_value = ord(word[j])\n",
    "            hash += ascii_value*(p**j)  # s[n-1].p^(n-1)\n",
    "        hash = hash % m\n",
    "        binary_string = bin(hash)[2:]\n",
    "        padded_binary_string = binary_string.zfill(8)\n",
    "        words_with_hash[word] = (word_dict[word],padded_binary_string)\n",
    "    \n",
    "    return words_with_hash\n",
    "computeHashValue(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfdded-ad70-4ce1-9c6c-976fb95ce186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e703194-ec96-4bbc-9082-79bf76a72ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compareSimhashes(simhash1,simhash2):\n",
    "#     commonSimhash = 0\n",
    "#     unique_count = 0\n",
    "#     pointer1 = 0\n",
    "#     pointer2 = 0\n",
    "#     while pointer1 < len(simhash1) and pointer2 < len(simhash2):\n",
    "#         if simhash1[pointer1] == simhash2[pointer2]:\n",
    "#             commonSimhash += 1\n",
    "#             pointer1 += 1\n",
    "#             pointer2 += 1\n",
    "#             unique_count += 1\n",
    "#         elif simhash1[pointer1] < simhash2[pointer2]:\n",
    "#             pointer1 += 1\n",
    "#             unique_count += 1\n",
    "#         else:\n",
    "#             pointer2 += 1\n",
    "#             unique_count += 1\n",
    "        \n",
    "#     return f\"The Percentage of duplicates is : {(commonSimhash/unique_count)*100} %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4718e817-016c-4abb-9d57-083dc2127ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 6, 0, 0, -2, -2, -2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeSimHash(original_text):\n",
    "    content = remove_panctuation(original_text)\n",
    "    word_dict = word_frequency(content)\n",
    "    hash64value = computeHashValue(word_dict)\n",
    "    weight_sum_vector = []\n",
    "    for i in range(8):\n",
    "        weight = 0\n",
    "        for j in hash64value:\n",
    "            if hash64value[j][1][i] == \"1\":\n",
    "                weight += hash64value[j][0]\n",
    "            else:\n",
    "                weight -= hash64value[j][0]\n",
    "        weight_sum_vector.append(weight)\n",
    "    print(weight_sum_vector)\n",
    "    fingerprint_vector = [1 if weight > 0 else 0 for weight in weight_sum_vector]\n",
    "    return fingerprint_vector\n",
    "computeSimHash(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b31df-057f-4979-aa33-b4bcde0cb04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93518166-4a2c-4cdd-ba4f-2b92f34a0555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a85ebb-a77f-4b97-8c71-84eb9f233de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
