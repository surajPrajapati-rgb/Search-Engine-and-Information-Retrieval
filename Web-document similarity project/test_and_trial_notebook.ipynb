{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e50f510-179a-42db-bf1a-046acdba22f1",
   "metadata": {},
   "source": [
    "- Building on your previous Python project\n",
    "- Count the frequency of every word (a word is a sequence of alphanumeric characters, case does NOT matter) in the body of your document\n",
    "- Write a 64 bit hash function for a word using polynomial rolling hash function\n",
    "       hash(s) = s[0] + s[1].p  + s[2].\n",
    "\n",
    "p^2   + ..... + s[n-1].p^(n-1)   mod n       \n",
    "\n",
    "Here s[i] is the ASCII for letter i in a word, use p = 53 and m = p2^64\n",
    "Compute Simhash for the document (as shown in slide 57)\n",
    "Modify your program to take two URLs from the web on the command line, print how many bits are common in their simhashes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9b6148-d65d-4688-b66b-dacc19965dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# original = \"Gen Gen AI is a predictive language model a translator that sits above existing unstructured data and seeks to generate content that a human would find pleasing. The data sets themselves first need to be rigorously processed and curated Gen, just as data scientists prepare data lakes for advanced analytics and analytical AI\"\n",
    "original = \"Tropical fish include fish the the the found in tropical environments around the world, including both freshwater and salt water species\"\n",
    "original = original.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e552cb-980f-40ff-8323-ada07109a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', ',', ':', ';',  '/', '\\\\', '!', '?', '(',')', '[', ']', '{', '}', '|', '&', '^', '%', '<', '>', '~', '#']\n",
    "def remove_panctuation(original):\n",
    "    cleaned_data = \"\"\n",
    "    for i in original:\n",
    "        if i not in punctuation:\n",
    "            cleaned_data +=i\n",
    "    return cleaned_data\n",
    "content = remove_panctuation(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5b1fde-2d08-4371-9fda-6301b51c69f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def word_frequency(content):\n",
    "    \n",
    "#     data = content.split()\n",
    "#     n = len(data)\n",
    "#     word_fre_dict = {}\n",
    "#     for word in range(0,n,2):\n",
    "#         ngram = \" \".join(data[word:word+5]) \n",
    "#         if word not in word_fre_dict:\n",
    "#             word_fre_dict[ngram] = 1\n",
    "#         else:\n",
    "#             word_fre_dict[ngram] += 1\n",
    "#     return word_fre_dict\n",
    "# word_dict = word_frequency(content)\n",
    "\n",
    "\n",
    "ngram = 1\n",
    "n_overlapping = 0\n",
    "def word_frequency(content):\n",
    "    data = content.split()\n",
    "    # print(data)\n",
    "    n = len(data)\n",
    "    word_fre_dict = {}\n",
    "    for word in range(0, n, ngram - n_overlapping):   # steping with ngram - n_overlapping steps for n overlapping\n",
    "        ngram_data = \" \".join(data[word:word+ngram])  # n gram collector\n",
    "        if ngram_data not in word_fre_dict:\n",
    "            word_fre_dict[ngram_data] = 1\n",
    "        else:\n",
    "            word_fre_dict[ngram_data] = word_fre_dict[ngram_data]+1\n",
    "            print(word_fre_dict[ngram_data])\n",
    "    return word_fre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ac8af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tropical': 2,\n",
       " 'fish': 2,\n",
       " 'include': 1,\n",
       " 'the': 4,\n",
       " 'found': 1,\n",
       " 'in': 1,\n",
       " 'environments': 1,\n",
       " 'around': 1,\n",
       " 'world,': 1,\n",
       " 'including': 1,\n",
       " 'both': 1,\n",
       " 'freshwater': 1,\n",
       " 'and': 1,\n",
       " 'salt': 1,\n",
       " 'water': 1,\n",
       " 'species': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b637ac12-b9b9-4bb0-b417-b55796cd5a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tropical fish include fish found': (1, '11111010'),\n",
       " 'fish include fish found in': (1, '11000111'),\n",
       " 'include fish found in tropical': (1, '11011111'),\n",
       " 'fish found in tropical environments': (1, '00010011'),\n",
       " 'found in tropical environments around': (1, '01111010'),\n",
       " 'in tropical environments around the': (1, '11111011'),\n",
       " 'tropical environments around the world': (1, '00000000'),\n",
       " 'environments around the world including': (1, '10101011'),\n",
       " 'around the world including both': (1, '01100100'),\n",
       " 'the world including both freshwater': (1, '10000010'),\n",
       " 'world including both freshwater and': (1, '10000100'),\n",
       " 'including both freshwater and salt': (1, '01001000'),\n",
       " 'both freshwater and salt water': (1, '01110010'),\n",
       " 'freshwater and salt water species': (1, '10101001'),\n",
       " 'and salt water species': (1, '00000110'),\n",
       " 'salt water species': (1, '11101011'),\n",
       " 'water species': (1, '00010111'),\n",
       " 'species': (1, '00000100')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeHashValue(word_dict):\n",
    "    p = 53\n",
    "    m = 2**8\n",
    "    words_with_hash = {}\n",
    "    for word in word_dict:\n",
    "        hash = 0\n",
    "        n=len(word)\n",
    "        for j in range(n):\n",
    "            ascii_value = ord(word[j])\n",
    "            hash += ascii_value*(p**j)  # s[n-1].p^(n-1)\n",
    "        hash = hash % m\n",
    "        binary_string = bin(hash)[2:]\n",
    "        padded_binary_string = binary_string.zfill(8)\n",
    "        words_with_hash[word] = (word_dict[word],padded_binary_string)\n",
    "    \n",
    "    return words_with_hash\n",
    "# computeHashValue(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfdded-ad70-4ce1-9c6c-976fb95ce186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e703194-ec96-4bbc-9082-79bf76a72ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compareSimhashes(simhash1,simhash2):\n",
    "#     commonSimhash = 0\n",
    "#     unique_count = 0\n",
    "#     pointer1 = 0\n",
    "#     pointer2 = 0\n",
    "#     while pointer1 < len(simhash1) and pointer2 < len(simhash2):\n",
    "#         if simhash1[pointer1] == simhash2[pointer2]:\n",
    "#             commonSimhash += 1\n",
    "#             pointer1 += 1\n",
    "#             pointer2 += 1\n",
    "#             unique_count += 1\n",
    "#         elif simhash1[pointer1] < simhash2[pointer2]:\n",
    "#             pointer1 += 1\n",
    "#             unique_count += 1\n",
    "#         else:\n",
    "#             pointer2 += 1\n",
    "#             unique_count += 1\n",
    "        \n",
    "#     return f\"The Percentage of duplicates is : {(commonSimhash/unique_count)*100} %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4718e817-016c-4abb-9d57-083dc2127ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 6, 0, 0, -2, -2, -2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeSimHash(original_text):\n",
    "    content = remove_panctuation(original_text)\n",
    "    word_dict = word_frequency(content)\n",
    "    hash64value = computeHashValue(word_dict)\n",
    "    weight_sum_vector = []\n",
    "    for i in range(8):\n",
    "        weight = 0\n",
    "        for j in hash64value:\n",
    "            if hash64value[j][1][i] == \"1\":\n",
    "                weight += hash64value[j][0]\n",
    "            else:\n",
    "                weight -= hash64value[j][0]\n",
    "        weight_sum_vector.append(weight)\n",
    "    print(weight_sum_vector)\n",
    "    fingerprint_vector = [1 if weight > 0 else 0 for weight in weight_sum_vector]\n",
    "    return fingerprint_vector\n",
    "computeSimHash(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00b31df-057f-4979-aa33-b4bcde0cb04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000000000000000000000000000010101010000000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"10101010000000\".zfill(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93518166-4a2c-4cdd-ba4f-2b92f34a0555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a85ebb-a77f-4b97-8c71-84eb9f233de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
